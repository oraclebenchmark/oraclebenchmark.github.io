[
  {
    "is_correct": {
      "total": {
        "correct": 95,
        "incorrect": 105,
        "accuracy": 47.5
      },
      "relation": {
        "correct": 43,
        "incorrect": 61,
        "accuracy": 41.34615384615385
      },
      "bound": {
        "correct": 52,
        "incorrect": 44,
        "accuracy": 54.166666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 50,
        "incorrect": 150,
        "accuracy": 25.0
      },
      "relation": {
        "correct": 25,
        "incorrect": 79,
        "accuracy": 24.03846153846154
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.041666666666668
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 18,
        "incorrect": 182,
        "accuracy": 9.0
      },
      "relation": {
        "correct": 11,
        "incorrect": 93,
        "accuracy": 10.576923076923077
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "approx_judge": {
      "total": {
        "correct": 179,
        "incorrect": 21,
        "accuracy": 89.5
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.61538461538461
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 188,
        "incorrect": 12,
        "accuracy": 94.0
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.15384615384616
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 11,
        "incorrect": 189,
        "accuracy": 5.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 5,
        "incorrect": 91,
        "accuracy": 5.208333333333334
      }
    },
    "all_true_indices": [
      "3",
      "6",
      "1",
      "35",
      "49",
      "98",
      "110",
      "158",
      "161",
      "193",
      "182"
    ],
    "Model": "Claude Opus 4",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://www.anthropic.com/news/claude-4",
    "Reasoning effort": "not applicable",
    "Date": "2025-05-14",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-06-12 00:45:16.535946"
  },
  {
    "is_correct": {
      "total": {
        "correct": 88,
        "incorrect": 112,
        "accuracy": 44.0
      },
      "relation": {
        "correct": 33,
        "incorrect": 71,
        "accuracy": 31.73076923076923
      },
      "bound": {
        "correct": 55,
        "incorrect": 41,
        "accuracy": 57.291666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 39,
        "incorrect": 161,
        "accuracy": 19.5
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.230769230769234
      },
      "bound": {
        "correct": 19,
        "incorrect": 77,
        "accuracy": 19.791666666666664
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 13,
        "incorrect": 187,
        "accuracy": 6.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "approx_judge": {
      "total": {
        "correct": 173,
        "incorrect": 27,
        "accuracy": 86.5
      },
      "relation": {
        "correct": 83,
        "incorrect": 21,
        "accuracy": 79.8076923076923
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "calc_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 101,
        "incorrect": 3,
        "accuracy": 97.11538461538461
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "all_true": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "all_true_indices": [
      "6",
      "23",
      "49",
      "92",
      "98",
      "153"
    ],
    "Model": "Claude Sonnet 4",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://www.anthropic.com/news/claude-4",
    "Reasoning effort": "not applicable",
    "Date": "2025-05-14",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-06-12 00:49:56.782607"
  },
  {
    "is_correct": {
      "total": {
        "correct": 109,
        "incorrect": 91,
        "accuracy": 54.50000000000001
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.230769230769226
      },
      "bound": {
        "correct": 63,
        "incorrect": 33,
        "accuracy": 65.625
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 35,
        "incorrect": 165,
        "accuracy": 17.5
      },
      "relation": {
        "correct": 22,
        "incorrect": 82,
        "accuracy": 21.153846153846153
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.541666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 30,
        "incorrect": 170,
        "accuracy": 15.0
      },
      "relation": {
        "correct": 15,
        "incorrect": 89,
        "accuracy": 14.423076923076922
      },
      "bound": {
        "correct": 15,
        "incorrect": 81,
        "accuracy": 15.625
      }
    },
    "approx_judge": {
      "total": {
        "correct": 126,
        "incorrect": 74,
        "accuracy": 63.0
      },
      "relation": {
        "correct": 66,
        "incorrect": 38,
        "accuracy": 63.46153846153846
      },
      "bound": {
        "correct": 60,
        "incorrect": 36,
        "accuracy": 62.5
      }
    },
    "calc_judge": {
      "total": {
        "correct": 189,
        "incorrect": 11,
        "accuracy": 94.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 14,
        "incorrect": 186,
        "accuracy": 7.000000000000001
      },
      "relation": {
        "correct": 8,
        "incorrect": 96,
        "accuracy": 7.6923076923076925
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.25
      }
    },
    "all_true_indices": [
      "6",
      "1",
      "23",
      "62",
      "76",
      "92",
      "107",
      "110",
      "128",
      "118",
      "143",
      "152",
      "153",
      "191"
    ],
    "Model": "DeepSeek-V3-0324",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "671B",
    "Source": "https://api-docs.deepseek.com/news/news250325",
    "Reasoning effort": "not applicable",
    "Date": "2025-03-25",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-06-12 03:09:08.485402"
  },
  {
    "is_correct": {
      "total": {
        "correct": 24,
        "incorrect": 176,
        "accuracy": 12.0
      },
      "relation": {
        "correct": 17,
        "incorrect": 87,
        "accuracy": 16.346153846153847
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 68,
        "incorrect": 132,
        "accuracy": 34.0
      },
      "relation": {
        "correct": 39,
        "incorrect": 65,
        "accuracy": 37.5
      },
      "bound": {
        "correct": 29,
        "incorrect": 67,
        "accuracy": 30.208333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 65,
        "incorrect": 135,
        "accuracy": 32.5
      },
      "relation": {
        "correct": 45,
        "incorrect": 59,
        "accuracy": 43.269230769230774
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.833333333333336
      }
    },
    "approx_judge": {
      "total": {
        "correct": 58,
        "incorrect": 142,
        "accuracy": 28.999999999999996
      },
      "relation": {
        "correct": 26,
        "incorrect": 78,
        "accuracy": 25.0
      },
      "bound": {
        "correct": 32,
        "incorrect": 64,
        "accuracy": 33.33333333333333
      }
    },
    "calc_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.58333333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 9,
        "incorrect": 191,
        "accuracy": 4.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "all_true_indices": [
      "6",
      "49",
      "62",
      "98",
      "153",
      "184",
      "186",
      "191",
      "189"
    ],
    "Model": "DeepSeek-R1-0528",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "671B",
    "Source": "https://api-docs.deepseek.com/news/news250528",
    "Reasoning effort": "not applicable",
    "Date": "2025-05-28",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-06-12 03:12:18.689983"
  },
  {
    "is_correct": {
      "total": {
        "correct": 146,
        "incorrect": 54,
        "accuracy": 73.0
      },
      "relation": {
        "correct": 71,
        "incorrect": 33,
        "accuracy": 68.26923076923077
      },
      "bound": {
        "correct": 75,
        "incorrect": 21,
        "accuracy": 78.125
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 98,
        "incorrect": 102,
        "accuracy": 49.0
      },
      "relation": {
        "correct": 51,
        "incorrect": 53,
        "accuracy": 49.03846153846153
      },
      "bound": {
        "correct": 47,
        "incorrect": 49,
        "accuracy": 48.95833333333333
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 104,
        "incorrect": 96,
        "accuracy": 52.0
      },
      "relation": {
        "correct": 63,
        "incorrect": 41,
        "accuracy": 60.57692307692307
      },
      "bound": {
        "correct": 41,
        "incorrect": 55,
        "accuracy": 42.70833333333333
      }
    },
    "approx_judge": {
      "total": {
        "correct": 40,
        "incorrect": 160,
        "accuracy": 20.0
      },
      "relation": {
        "correct": 18,
        "incorrect": 86,
        "accuracy": 17.307692307692307
      },
      "bound": {
        "correct": 22,
        "incorrect": 74,
        "accuracy": 22.916666666666664
      }
    },
    "calc_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.58333333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 19,
        "incorrect": 181,
        "accuracy": 9.5
      },
      "relation": {
        "correct": 12,
        "incorrect": 92,
        "accuracy": 11.538461538461538
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "all_true_indices": [
      "0",
      "7",
      "28",
      "49",
      "68",
      "76",
      "89",
      "98",
      "112",
      "131",
      "120",
      "129",
      "153",
      "158",
      "161",
      "175",
      "180",
      "191",
      "193"
    ],
    "Model": "DeepSeek-R1-0528",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "671B",
    "Source": "https://api-docs.deepseek.com/news/news250528",
    "Reasoning effort": "not applicable",
    "Date": "2025-05-28",
    "Max Tokens": 40000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-06-12 22:26:20.132860"
  },
  {
    "is_correct": {
      "total": {
        "correct": 90,
        "incorrect": 110,
        "accuracy": 45.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.42307692307692
      },
      "bound": {
        "correct": 49,
        "incorrect": 47,
        "accuracy": 51.041666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 166,
        "incorrect": 34,
        "accuracy": 83.0
      },
      "relation": {
        "correct": 89,
        "incorrect": 15,
        "accuracy": 85.57692307692307
      },
      "bound": {
        "correct": 77,
        "incorrect": 19,
        "accuracy": 80.20833333333334
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 88,
        "incorrect": 112,
        "accuracy": 44.0
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.19230769230769
      },
      "bound": {
        "correct": 41,
        "incorrect": 55,
        "accuracy": 42.70833333333333
      }
    },
    "approx_judge": {
      "total": {
        "correct": 178,
        "incorrect": 22,
        "accuracy": 89.0
      },
      "relation": {
        "correct": 90,
        "incorrect": 14,
        "accuracy": 86.53846153846155
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 192,
        "incorrect": 8,
        "accuracy": 96.0
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.07692307692307
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "all_true": {
      "total": {
        "correct": 55,
        "incorrect": 145,
        "accuracy": 27.500000000000004
      },
      "relation": {
        "correct": 28,
        "incorrect": 76,
        "accuracy": 26.923076923076923
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.125
      }
    },
    "all_true_indices": [
      "6",
      "3",
      "1",
      "19",
      "14",
      "11",
      "23",
      "21",
      "24",
      "20",
      "34",
      "46",
      "42",
      "56",
      "59",
      "73",
      "76",
      "66",
      "78",
      "75",
      "92",
      "82",
      "86",
      "94",
      "88",
      "90",
      "99",
      "97",
      "89",
      "100",
      "98",
      "96",
      "103",
      "106",
      "110",
      "112",
      "108",
      "121",
      "134",
      "131",
      "143",
      "153",
      "157",
      "161",
      "158",
      "160",
      "156",
      "168",
      "175",
      "189",
      "193",
      "190",
      "184",
      "191",
      "188"
    ],
    "Model": "Gemini 2.5 Flash Preview 05-20",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview-05-20",
    "Reasoning effort": "not applicable",
    "Date": "2025-05-20",
    "Max Tokens": 40000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-06-13 23:52:28.624851"
  },
  {
    "is_correct": {
      "total": {
        "correct": 132,
        "incorrect": 68,
        "accuracy": 66.0
      },
      "relation": {
        "correct": 57,
        "incorrect": 47,
        "accuracy": 54.807692307692314
      },
      "bound": {
        "correct": 75,
        "incorrect": 21,
        "accuracy": 78.125
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 170,
        "incorrect": 30,
        "accuracy": 85.0
      },
      "relation": {
        "correct": 85,
        "incorrect": 19,
        "accuracy": 81.73076923076923
      },
      "bound": {
        "correct": 85,
        "incorrect": 11,
        "accuracy": 88.54166666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 130,
        "incorrect": 70,
        "accuracy": 65.0
      },
      "relation": {
        "correct": 64,
        "incorrect": 40,
        "accuracy": 61.53846153846154
      },
      "bound": {
        "correct": 66,
        "incorrect": 30,
        "accuracy": 68.75
      }
    },
    "approx_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 93,
        "incorrect": 11,
        "accuracy": 89.42307692307693
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 195,
        "incorrect": 5,
        "accuracy": 97.5
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.15384615384616
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 92,
        "incorrect": 108,
        "accuracy": 46.0
      },
      "relation": {
        "correct": 39,
        "incorrect": 65,
        "accuracy": 37.5
      },
      "bound": {
        "correct": 53,
        "incorrect": 43,
        "accuracy": 55.208333333333336
      }
    },
    "all_true_indices": [
      "1",
      "3",
      "6",
      "16",
      "14",
      "7",
      "11",
      "8",
      "19",
      "31",
      "12",
      "27",
      "21",
      "30",
      "35",
      "0",
      "34",
      "9",
      "10",
      "24",
      "32",
      "22",
      "58",
      "42",
      "17",
      "49",
      "59",
      "61",
      "52",
      "23",
      "55",
      "62",
      "64",
      "63",
      "68",
      "76",
      "45",
      "79",
      "75",
      "92",
      "73",
      "80",
      "81",
      "86",
      "82",
      "88",
      "78",
      "39",
      "72",
      "74",
      "89",
      "100",
      "95",
      "98",
      "96",
      "105",
      "93",
      "121",
      "104",
      "101",
      "108",
      "107",
      "118",
      "112",
      "111",
      "119",
      "106",
      "134",
      "132",
      "135",
      "131",
      "143",
      "152",
      "156",
      "153",
      "147",
      "157",
      "161",
      "160",
      "162",
      "166",
      "178",
      "175",
      "177",
      "189",
      "186",
      "190",
      "170",
      "184",
      "188",
      "199",
      "193"
    ],
    "Model": "Gemini 2.5 Pro Preview",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro-preview-06-05",
    "Reasoning effort": "not applicable",
    "Date": "2025-06-05",
    "Max Tokens": 40000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-06-14 00:57:36.831162"
  },
  {
    "is_correct": {
      "total": {
        "correct": 26,
        "incorrect": 174,
        "accuracy": 13.0
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.833333333333336
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 92,
        "incorrect": 12,
        "accuracy": 88.46153846153845
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 40,
        "incorrect": 160,
        "accuracy": 20.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.192307692307693
      },
      "bound": {
        "correct": 19,
        "incorrect": 77,
        "accuracy": 19.791666666666664
      }
    },
    "approx_judge": {
      "total": {
        "correct": 196,
        "incorrect": 4,
        "accuracy": 98.0
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.07692307692307
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 197,
        "incorrect": 3,
        "accuracy": 98.5
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.07692307692307
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 20,
        "incorrect": 180,
        "accuracy": 10.0
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 14,
        "incorrect": 82,
        "accuracy": 14.583333333333334
      }
    },
    "all_true_indices": [
      "6",
      "3",
      "1",
      "14",
      "23",
      "35",
      "28",
      "20",
      "31",
      "49",
      "59",
      "76",
      "58",
      "92",
      "98",
      "134",
      "121",
      "131",
      "161",
      "189"
    ],
    "Model": "Gemini 2.5 Pro Preview",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro-preview-06-05",
    "Reasoning effort": "not applicable",
    "Date": "2025-06-05",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-06-14 00:59:39.880539"
  },
  {
    "is_correct": {
      "total": {
        "correct": 35,
        "incorrect": 165,
        "accuracy": 17.5
      },
      "relation": {
        "correct": 22,
        "incorrect": 82,
        "accuracy": 21.153846153846153
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.541666666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 164,
        "incorrect": 36,
        "accuracy": 82.0
      },
      "relation": {
        "correct": 87,
        "incorrect": 17,
        "accuracy": 83.65384615384616
      },
      "bound": {
        "correct": 77,
        "incorrect": 19,
        "accuracy": 80.20833333333334
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 62,
        "incorrect": 138,
        "accuracy": 31.0
      },
      "relation": {
        "correct": 38,
        "incorrect": 66,
        "accuracy": 36.53846153846153
      },
      "bound": {
        "correct": 24,
        "incorrect": 72,
        "accuracy": 25.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 193,
        "incorrect": 7,
        "accuracy": 96.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "all_true": {
      "total": {
        "correct": 29,
        "incorrect": 171,
        "accuracy": 14.499999999999998
      },
      "relation": {
        "correct": 17,
        "incorrect": 87,
        "accuracy": 16.346153846153847
      },
      "bound": {
        "correct": 12,
        "incorrect": 84,
        "accuracy": 12.5
      }
    },
    "all_true_indices": [
      "16",
      "3",
      "1",
      "14",
      "6",
      "23",
      "21",
      "33",
      "34",
      "49",
      "59",
      "99",
      "92",
      "110",
      "106",
      "98",
      "121",
      "134",
      "131",
      "143",
      "161",
      "158",
      "153",
      "175",
      "184",
      "189",
      "193",
      "185",
      "191"
    ],
    "Model": "Gemini 2.5 Flash Preview 05-20",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview-05-20",
    "Reasoning effort": "not applicable",
    "Date": "2025-05-20",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-06-14 02:13:04.549577"
  },
  {
    "is_correct": {
      "total": {
        "correct": 152,
        "incorrect": 48,
        "accuracy": 76.0
      },
      "relation": {
        "correct": 74,
        "incorrect": 30,
        "accuracy": 71.15384615384616
      },
      "bound": {
        "correct": 78,
        "incorrect": 18,
        "accuracy": 81.25
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 114,
        "incorrect": 86,
        "accuracy": 56.99999999999999
      },
      "relation": {
        "correct": 75,
        "incorrect": 29,
        "accuracy": 72.11538461538461
      },
      "bound": {
        "correct": 39,
        "incorrect": 57,
        "accuracy": 40.625
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 28,
        "incorrect": 172,
        "accuracy": 14.000000000000002
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.230769230769234
      },
      "bound": {
        "correct": 8,
        "incorrect": 88,
        "accuracy": 8.333333333333332
      }
    },
    "approx_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 193,
        "incorrect": 7,
        "accuracy": 96.5
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.15384615384616
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "all_true": {
      "total": {
        "correct": 16,
        "incorrect": 184,
        "accuracy": 8.0
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.653846153846153
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "all_true_indices": [
      "3",
      "6",
      "14",
      "28",
      "35",
      "49",
      "91",
      "98",
      "110",
      "124",
      "134",
      "161",
      "157",
      "180",
      "193",
      "191"
    ],
    "Model": "Grok 4",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://docs.x.ai/docs/models/grok-4-0709",
    "Reasoning effort": "not applicable",
    "Date": "2025-07-09",
    "Max Tokens": 40000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-07-12 21:14:12.977858"
  },
  {
    "is_correct": {
      "total": {
        "correct": 125,
        "incorrect": 75,
        "accuracy": 62.5
      },
      "relation": {
        "correct": 52,
        "incorrect": 52,
        "accuracy": 50.0
      },
      "bound": {
        "correct": 73,
        "incorrect": 23,
        "accuracy": 76.04166666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 67,
        "incorrect": 133,
        "accuracy": 33.5
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.42307692307692
      },
      "bound": {
        "correct": 26,
        "incorrect": 70,
        "accuracy": 27.083333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 51,
        "incorrect": 149,
        "accuracy": 25.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.69230769230769
      },
      "bound": {
        "correct": 17,
        "incorrect": 79,
        "accuracy": 17.708333333333336
      }
    },
    "approx_judge": {
      "total": {
        "correct": 105,
        "incorrect": 95,
        "accuracy": 52.5
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.19230769230769
      },
      "bound": {
        "correct": 58,
        "incorrect": 38,
        "accuracy": 60.416666666666664
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 18,
        "incorrect": 182,
        "accuracy": 9.0
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.653846153846153
      },
      "bound": {
        "correct": 9,
        "incorrect": 87,
        "accuracy": 9.375
      }
    },
    "all_true_indices": [
      "6",
      "14",
      "19",
      "49",
      "59",
      "62",
      "66",
      "82",
      "84",
      "121",
      "129",
      "134",
      "143",
      "153",
      "157",
      "161",
      "184",
      "188"
    ],
    "Model": "Kimi K2 Instruct",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "1000B",
    "Source": "https://huggingface.co/moonshotai/Kimi-K2-Instruct",
    "Reasoning effort": "not applicable",
    "Date": "2025-07-12",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-07-20 20:07:09.372066"
  },
  {
    "is_correct": {
      "total": {
        "correct": 88,
        "incorrect": 112,
        "accuracy": 44.0
      },
      "relation": {
        "correct": 40,
        "incorrect": 64,
        "accuracy": 38.46153846153847
      },
      "bound": {
        "correct": 48,
        "incorrect": 48,
        "accuracy": 50.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.192307692307693
      },
      "bound": {
        "correct": 21,
        "incorrect": 75,
        "accuracy": 21.875
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 26,
        "incorrect": 78,
        "accuracy": 25.0
      },
      "bound": {
        "correct": 16,
        "incorrect": 80,
        "accuracy": 16.666666666666664
      }
    },
    "approx_judge": {
      "total": {
        "correct": 64,
        "incorrect": 136,
        "accuracy": 32.0
      },
      "relation": {
        "correct": 39,
        "incorrect": 65,
        "accuracy": 37.5
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.041666666666668
      }
    },
    "calc_judge": {
      "total": {
        "correct": 171,
        "incorrect": 29,
        "accuracy": 85.5
      },
      "relation": {
        "correct": 89,
        "incorrect": 15,
        "accuracy": 85.57692307692307
      },
      "bound": {
        "correct": 82,
        "incorrect": 14,
        "accuracy": 85.41666666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 11,
        "incorrect": 189,
        "accuracy": 5.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 5,
        "incorrect": 91,
        "accuracy": 5.208333333333334
      }
    },
    "all_true_indices": [
      "6",
      "14",
      "49",
      "62",
      "84",
      "98",
      "118",
      "135",
      "143",
      "161",
      "184"
    ],
    "Model": "Qwen3-4B",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "4B",
    "Source": "https://huggingface.co/Qwen/Qwen3-4B",
    "Reasoning effort": "not applicable",
    "Date": "2025-04-27",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-07-20 21:02:39.689619"
  },
  {
    "is_correct": {
      "total": {
        "correct": 133,
        "incorrect": 67,
        "accuracy": 66.5
      },
      "relation": {
        "correct": 74,
        "incorrect": 30,
        "accuracy": 71.15384615384616
      },
      "bound": {
        "correct": 59,
        "incorrect": 37,
        "accuracy": 61.458333333333336
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 80,
        "incorrect": 120,
        "accuracy": 40.0
      },
      "relation": {
        "correct": 42,
        "incorrect": 62,
        "accuracy": 40.38461538461539
      },
      "bound": {
        "correct": 38,
        "incorrect": 58,
        "accuracy": 39.58333333333333
      }
    },
    "approx_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "calc_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 101,
        "incorrect": 3,
        "accuracy": 97.11538461538461
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.58333333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 47,
        "incorrect": 153,
        "accuracy": 23.5
      },
      "relation": {
        "correct": 25,
        "incorrect": 79,
        "accuracy": 24.03846153846154
      },
      "bound": {
        "correct": 22,
        "incorrect": 74,
        "accuracy": 22.916666666666664
      }
    },
    "all_true_indices": [
      "4",
      "3",
      "6",
      "14",
      "16",
      "23",
      "21",
      "35",
      "42",
      "47",
      "49",
      "52",
      "71",
      "66",
      "76",
      "75",
      "79",
      "84",
      "82",
      "78",
      "90",
      "89",
      "98",
      "97",
      "100",
      "99",
      "110",
      "112",
      "118",
      "121",
      "120",
      "134",
      "135",
      "143",
      "153",
      "158",
      "160",
      "164",
      "166",
      "175",
      "182",
      "189",
      "184",
      "187",
      "193",
      "191",
      "198"
    ],
    "Model": "gpt-oss-120b",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "120B",
    "Source": "https://huggingface.co/openai/gpt-oss-120b",
    "Reasoning effort": "not applicable",
    "Date": "2025-08-05",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-08-06 06:44:21.573012"
  },
  {
    "is_correct": {
      "total": {
        "correct": 73,
        "incorrect": 127,
        "accuracy": 36.5
      },
      "relation": {
        "correct": 40,
        "incorrect": 64,
        "accuracy": 38.46153846153847
      },
      "bound": {
        "correct": 33,
        "incorrect": 63,
        "accuracy": 34.375
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 189,
        "incorrect": 11,
        "accuracy": 94.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 94,
        "incorrect": 106,
        "accuracy": 47.0
      },
      "relation": {
        "correct": 50,
        "incorrect": 54,
        "accuracy": 48.07692307692308
      },
      "bound": {
        "correct": 44,
        "incorrect": 52,
        "accuracy": 45.83333333333333
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 94,
        "incorrect": 10,
        "accuracy": 90.38461538461539
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "calc_judge": {
      "total": {
        "correct": 194,
        "incorrect": 6,
        "accuracy": 97.0
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.03846153846155
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 56,
        "incorrect": 144,
        "accuracy": 28.000000000000004
      },
      "relation": {
        "correct": 25,
        "incorrect": 79,
        "accuracy": 24.03846153846154
      },
      "bound": {
        "correct": 31,
        "incorrect": 65,
        "accuracy": 32.29166666666667
      }
    },
    "all_true_indices": [
      "3",
      "6",
      "1",
      "0",
      "8",
      "13",
      "14",
      "19",
      "23",
      "20",
      "24",
      "34",
      "30",
      "45",
      "49",
      "59",
      "55",
      "58",
      "62",
      "76",
      "68",
      "66",
      "70",
      "71",
      "79",
      "90",
      "88",
      "92",
      "89",
      "94",
      "93",
      "98",
      "99",
      "110",
      "103",
      "121",
      "106",
      "134",
      "129",
      "143",
      "152",
      "153",
      "158",
      "161",
      "175",
      "180",
      "168",
      "160",
      "166",
      "182",
      "184",
      "189",
      "193",
      "186",
      "188",
      "191"
    ],
    "Model": "GPT-5",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://platform.openai.com/docs/models/gpt-5",
    "Reasoning effort": "medium",
    "Date": "2025-08-07",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-08-08 07:16:02.560565"
  },
  {
    "is_correct": {
      "total": {
        "correct": 111,
        "incorrect": 89,
        "accuracy": 55.50000000000001
      },
      "relation": {
        "correct": 60,
        "incorrect": 44,
        "accuracy": 57.692307692307686
      },
      "bound": {
        "correct": 51,
        "incorrect": 45,
        "accuracy": 53.125
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 100,
        "incorrect": 100,
        "accuracy": 50.0
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.769230769230774
      },
      "bound": {
        "correct": 42,
        "incorrect": 54,
        "accuracy": 43.75
      }
    },
    "approx_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.61538461538461
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 197,
        "incorrect": 3,
        "accuracy": 98.5
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.07692307692307
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 61,
        "incorrect": 139,
        "accuracy": 30.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.69230769230769
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.125
      }
    },
    "all_true_indices": [
      "3",
      "6",
      "1",
      "14",
      "20",
      "21",
      "23",
      "22",
      "27",
      "32",
      "35",
      "42",
      "34",
      "55",
      "49",
      "59",
      "51",
      "57",
      "62",
      "75",
      "76",
      "79",
      "78",
      "84",
      "90",
      "92",
      "89",
      "97",
      "98",
      "99",
      "105",
      "101",
      "104",
      "106",
      "110",
      "121",
      "129",
      "134",
      "132",
      "137",
      "143",
      "152",
      "153",
      "154",
      "156",
      "157",
      "161",
      "158",
      "155",
      "162",
      "168",
      "175",
      "178",
      "180",
      "184",
      "186",
      "189",
      "190",
      "188",
      "193",
      "191"
    ],
    "Model": "GPT-5 mini",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://platform.openai.com/docs/models/gpt-5-mini",
    "Reasoning effort": "medium",
    "Date": "2025-08-07",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-08-08 07:16:45.520846"
  },
  {
    "is_correct": {
      "total": {
        "correct": 133,
        "incorrect": 67,
        "accuracy": 66.5
      },
      "relation": {
        "correct": 71,
        "incorrect": 33,
        "accuracy": 68.26923076923077
      },
      "bound": {
        "correct": 62,
        "incorrect": 34,
        "accuracy": 64.58333333333334
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 138,
        "incorrect": 62,
        "accuracy": 69.0
      },
      "relation": {
        "correct": 72,
        "incorrect": 32,
        "accuracy": 69.23076923076923
      },
      "bound": {
        "correct": 66,
        "incorrect": 30,
        "accuracy": 68.75
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 93,
        "incorrect": 11,
        "accuracy": 89.42307692307693
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 196,
        "incorrect": 4,
        "accuracy": 98.0
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.15384615384616
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "all_true": {
      "total": {
        "correct": 94,
        "incorrect": 106,
        "accuracy": 47.0
      },
      "relation": {
        "correct": 43,
        "incorrect": 61,
        "accuracy": 41.34615384615385
      },
      "bound": {
        "correct": 51,
        "incorrect": 45,
        "accuracy": 53.125
      }
    },
    "all_true_indices": [
      "3",
      "1",
      "6",
      "7",
      "0",
      "8",
      "13",
      "14",
      "16",
      "19",
      "23",
      "22",
      "20",
      "30",
      "34",
      "33",
      "35",
      "32",
      "31",
      "39",
      "42",
      "44",
      "45",
      "47",
      "52",
      "49",
      "56",
      "55",
      "57",
      "62",
      "61",
      "60",
      "64",
      "72",
      "65",
      "66",
      "73",
      "71",
      "76",
      "79",
      "81",
      "75",
      "84",
      "88",
      "86",
      "90",
      "93",
      "92",
      "89",
      "94",
      "97",
      "95",
      "100",
      "105",
      "103",
      "107",
      "110",
      "112",
      "115",
      "119",
      "111",
      "118",
      "121",
      "129",
      "132",
      "134",
      "137",
      "143",
      "152",
      "153",
      "156",
      "157",
      "158",
      "161",
      "160",
      "162",
      "168",
      "169",
      "175",
      "170",
      "178",
      "180",
      "182",
      "184",
      "183",
      "189",
      "188",
      "186",
      "187",
      "190",
      "193",
      "197",
      "198",
      "199"
    ],
    "Model": "GPT-5",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://platform.openai.com/docs/models/gpt-5",
    "Reasoning effort": "medium",
    "Date": "2025-08-07",
    "Max Tokens": 30000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-08-08 07:59:45.464547"
  },
  {
    "is_correct": {
      "total": {
        "correct": 84,
        "incorrect": 116,
        "accuracy": 42.0
      },
      "relation": {
        "correct": 39,
        "incorrect": 65,
        "accuracy": 37.5
      },
      "bound": {
        "correct": 45,
        "incorrect": 51,
        "accuracy": 46.875
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 98,
        "incorrect": 102,
        "accuracy": 49.0
      },
      "relation": {
        "correct": 63,
        "incorrect": 41,
        "accuracy": 60.57692307692307
      },
      "bound": {
        "correct": 35,
        "incorrect": 61,
        "accuracy": 36.45833333333333
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 8,
        "incorrect": 192,
        "accuracy": 4.0
      },
      "relation": {
        "correct": 5,
        "incorrect": 99,
        "accuracy": 4.807692307692308
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 87,
        "incorrect": 9,
        "accuracy": 90.625
      }
    },
    "all_true": {
      "total": {
        "correct": 4,
        "incorrect": 196,
        "accuracy": 2.0
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "106_result.json",
      "253_result.json",
      "278_result.json",
      "130_result.json"
    ],
    "Model": "Claude 3.7 Sonnet",
    "Type": "Reasoning",
    "Source": "https://www.anthropic.com/news/claude-3-7-sonnet",
    "Date": "2025-02-19",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 83,
        "incorrect": 117,
        "accuracy": 41.5
      },
      "relation": {
        "correct": 35,
        "incorrect": 69,
        "accuracy": 33.65384615384615
      },
      "bound": {
        "correct": 48,
        "incorrect": 48,
        "accuracy": 50.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 98,
        "incorrect": 102,
        "accuracy": 49.0
      },
      "relation": {
        "correct": 66,
        "incorrect": 38,
        "accuracy": 63.46153846153846
      },
      "bound": {
        "correct": 32,
        "incorrect": 64,
        "accuracy": 33.33333333333333
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 93,
        "incorrect": 11,
        "accuracy": 89.42307692307693
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 184,
        "incorrect": 16,
        "accuracy": 92.0
      },
      "relation": {
        "correct": 96,
        "incorrect": 8,
        "accuracy": 92.3076923076923
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 2,
        "incorrect": 198,
        "accuracy": 1.0
      },
      "relation": {
        "correct": 1,
        "incorrect": 103,
        "accuracy": 0.9615384615384616
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0416666666666665
      }
    },
    "all_true_indices": [
      "258_result.json",
      "106_result.json"
    ],
    "Model": "Claude 3.7 Sonnet",
    "Type": "Reasoning",
    "Source": "https://www.anthropic.com/news/claude-3-7-sonnet",
    "Date": "2025-02-19",
    "Max Tokens": "8000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 99,
        "incorrect": 101,
        "accuracy": 49.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.69230769230769
      },
      "bound": {
        "correct": 65,
        "incorrect": 31,
        "accuracy": 67.70833333333334
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 114,
        "incorrect": 86,
        "accuracy": 56.99999999999999
      },
      "relation": {
        "correct": 63,
        "incorrect": 41,
        "accuracy": 60.57692307692307
      },
      "bound": {
        "correct": 51,
        "incorrect": 45,
        "accuracy": 53.125
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 35,
        "incorrect": 165,
        "accuracy": 17.5
      },
      "relation": {
        "correct": 29,
        "incorrect": 75,
        "accuracy": 27.884615384615387
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.25
      }
    },
    "approx_judge": {
      "total": {
        "correct": 162,
        "incorrect": 38,
        "accuracy": 81.0
      },
      "relation": {
        "correct": 70,
        "incorrect": 34,
        "accuracy": 67.3076923076923
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 10,
        "incorrect": 190,
        "accuracy": 5.0
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "all_true_indices": [
      "270_result.json",
      "284_result.json",
      "192_result.json",
      "266_result.json",
      "106_result.json",
      "218_result.json",
      "253_result.json",
      "114_result.json",
      "123_result.json",
      "243_result.json"
    ],
    "Model": "DeepSeek-R1",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
    "Date": "2025-01-19",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "671B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 66,
        "incorrect": 134,
        "accuracy": 33.0
      },
      "relation": {
        "correct": 24,
        "incorrect": 80,
        "accuracy": 23.076923076923077
      },
      "bound": {
        "correct": 42,
        "incorrect": 54,
        "accuracy": 43.75
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 23,
        "incorrect": 177,
        "accuracy": 11.5
      },
      "relation": {
        "correct": 12,
        "incorrect": 92,
        "accuracy": 11.538461538461538
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.458333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5000000000000004
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "approx_judge": {
      "total": {
        "correct": 146,
        "incorrect": 54,
        "accuracy": 73.0
      },
      "relation": {
        "correct": 72,
        "incorrect": 32,
        "accuracy": 69.23076923076923
      },
      "bound": {
        "correct": 74,
        "incorrect": 22,
        "accuracy": 77.08333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.26923076923077
      },
      "bound": {
        "correct": 84,
        "incorrect": 12,
        "accuracy": 87.5
      }
    },
    "all_true": {
      "total": {
        "correct": 3,
        "incorrect": 197,
        "accuracy": 1.5
      },
      "relation": {
        "correct": 1,
        "incorrect": 103,
        "accuracy": 0.9615384615384616
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "103_result.json",
      "106_result.json",
      "261_result.json"
    ],
    "Model": "Gemini 2.0 Flash-Lite",
    "Type": "Chat",
    "Source": "https://deepmind.google/technologies/gemini/flash-lite/",
    "Date": "2025-02-25",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 98,
        "incorrect": 102,
        "accuracy": 49.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.42307692307692
      },
      "bound": {
        "correct": 57,
        "incorrect": 39,
        "accuracy": 59.375
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 31,
        "incorrect": 169,
        "accuracy": 15.5
      },
      "relation": {
        "correct": 18,
        "incorrect": 86,
        "accuracy": 17.307692307692307
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.541666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 27,
        "incorrect": 173,
        "accuracy": 13.5
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.230769230769234
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "approx_judge": {
      "total": {
        "correct": 111,
        "incorrect": 89,
        "accuracy": 55.50000000000001
      },
      "relation": {
        "correct": 53,
        "incorrect": 51,
        "accuracy": 50.96153846153846
      },
      "bound": {
        "correct": 58,
        "incorrect": 38,
        "accuracy": 60.416666666666664
      }
    },
    "calc_judge": {
      "total": {
        "correct": 189,
        "incorrect": 11,
        "accuracy": 94.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "all_true_indices": [
      "198_result.json",
      "103_result.json",
      "106_result.json",
      "261_result.json",
      "243_result.json",
      "149_result.json"
    ],
    "Model": "Gemini 2.0 Flash",
    "Type": "Chat",
    "Source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash",
    "Date": "2025-02-05",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 11,
        "incorrect": 189,
        "accuracy": 5.5
      },
      "relation": {
        "correct": 7,
        "incorrect": 97,
        "accuracy": 6.730769230769231
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 176,
        "incorrect": 24,
        "accuracy": 88.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 81,
        "incorrect": 15,
        "accuracy": 84.375
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 27,
        "incorrect": 173,
        "accuracy": 13.5
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.230769230769234
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "approx_judge": {
      "total": {
        "correct": 200,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 200,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "all_true": {
      "total": {
        "correct": 9,
        "incorrect": 191,
        "accuracy": 4.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "all_true_indices": [
      "198_result.json",
      "205_result.json",
      "103_result.json",
      "234_result.json",
      "106_result.json",
      "218_result.json",
      "253_result.json",
      "261_result.json",
      "149_result.json"
    ],
    "Model": "Gemini 2.5 Flash",
    "Type": "Reasoning",
    "Source": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash",
    "Date": "2025-04-17",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 89,
        "incorrect": 111,
        "accuracy": 44.5
      },
      "relation": {
        "correct": 43,
        "incorrect": 61,
        "accuracy": 41.34615384615385
      },
      "bound": {
        "correct": 46,
        "incorrect": 50,
        "accuracy": 47.91666666666667
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 162,
        "incorrect": 38,
        "accuracy": 81.0
      },
      "relation": {
        "correct": 92,
        "incorrect": 12,
        "accuracy": 88.46153846153845
      },
      "bound": {
        "correct": 70,
        "incorrect": 26,
        "accuracy": 72.91666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 73,
        "incorrect": 127,
        "accuracy": 36.5
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.230769230769226
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.125
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 195,
        "incorrect": 5,
        "accuracy": 97.5
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.07692307692307
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "all_true": {
      "total": {
        "correct": 47,
        "incorrect": 153,
        "accuracy": 23.5
      },
      "relation": {
        "correct": 25,
        "incorrect": 79,
        "accuracy": 24.03846153846154
      },
      "bound": {
        "correct": 22,
        "incorrect": 74,
        "accuracy": 22.916666666666664
      }
    },
    "all_true_indices": [
      "232_result.json",
      "119_result.json",
      "175_result.json",
      "152_result.json",
      "284_result.json",
      "198_result.json",
      "210_result.json",
      "260_result.json",
      "179_result.json",
      "237_result.json",
      "269_result.json",
      "205_result.json",
      "288_result.json",
      "275_result.json",
      "170_result.json",
      "103_result.json",
      "258_result.json",
      "206_result.json",
      "192_result.json",
      "234_result.json",
      "221_result.json",
      "166_result.json",
      "121_result.json",
      "266_result.json",
      "158_result.json",
      "128_result.json",
      "176_result.json",
      "106_result.json",
      "197_result.json",
      "289_result.json",
      "101_result.json",
      "218_result.json",
      "156_result.json",
      "253_result.json",
      "199_result.json",
      "114_result.json",
      "261_result.json",
      "243_result.json",
      "145_result.json",
      "293_result.json",
      "135_result.json",
      "107_result.json",
      "120_result.json",
      "149_result.json",
      "212_result.json",
      "130_result.json",
      "229_result.json"
    ],
    "Model": "Gemini 2.5 Flash",
    "Type": "Reasoning",
    "Source": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash",
    "Date": "2025-04-17",
    "Max Tokens": "40000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 14,
        "incorrect": 186,
        "accuracy": 7.000000000000001
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 8,
        "incorrect": 88,
        "accuracy": 8.333333333333332
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 177,
        "incorrect": 23,
        "accuracy": 88.5
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.26923076923077
      },
      "bound": {
        "correct": 80,
        "incorrect": 16,
        "accuracy": 83.33333333333334
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 38,
        "incorrect": 162,
        "accuracy": 19.0
      },
      "relation": {
        "correct": 26,
        "incorrect": 78,
        "accuracy": 25.0
      },
      "bound": {
        "correct": 12,
        "incorrect": 84,
        "accuracy": 12.5
      }
    },
    "approx_judge": {
      "total": {
        "correct": 200,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 199,
        "incorrect": 1,
        "accuracy": 99.5
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.03846153846155
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "all_true": {
      "total": {
        "correct": 12,
        "incorrect": 188,
        "accuracy": 6.0
      },
      "relation": {
        "correct": 5,
        "incorrect": 99,
        "accuracy": 4.807692307692308
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "all_true_indices": [
      "103_result.json",
      "234_result.json",
      "221_result.json",
      "116_result.json",
      "163_result.json",
      "128_result.json",
      "106_result.json",
      "231_result.json",
      "289_result.json",
      "261_result.json",
      "135_result.json",
      "149_result.json"
    ],
    "Model": "Gemini 2.5 Pro",
    "Type": "Reasoning",
    "Source": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro",
    "Date": "2025-03-25",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 136,
        "incorrect": 64,
        "accuracy": 68.0
      },
      "relation": {
        "correct": 64,
        "incorrect": 40,
        "accuracy": 61.53846153846154
      },
      "bound": {
        "correct": 72,
        "incorrect": 24,
        "accuracy": 75.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 175,
        "incorrect": 25,
        "accuracy": 87.5
      },
      "relation": {
        "correct": 90,
        "incorrect": 14,
        "accuracy": 86.53846153846155
      },
      "bound": {
        "correct": 85,
        "incorrect": 11,
        "accuracy": 88.54166666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 126,
        "incorrect": 74,
        "accuracy": 63.0
      },
      "relation": {
        "correct": 68,
        "incorrect": 36,
        "accuracy": 65.38461538461539
      },
      "bound": {
        "correct": 58,
        "incorrect": 38,
        "accuracy": 60.416666666666664
      }
    },
    "approx_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 90,
        "incorrect": 14,
        "accuracy": 86.53846153846155
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 196,
        "incorrect": 4,
        "accuracy": 98.0
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.07692307692307
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 87,
        "incorrect": 113,
        "accuracy": 43.5
      },
      "relation": {
        "correct": 40,
        "incorrect": 64,
        "accuracy": 38.46153846153847
      },
      "bound": {
        "correct": 47,
        "incorrect": 49,
        "accuracy": 48.95833333333333
      }
    },
    "all_true_indices": [
      "291_result.json",
      "119_result.json",
      "188_result.json",
      "270_result.json",
      "200_result.json",
      "298_result.json",
      "110_result.json",
      "227_result.json",
      "257_result.json",
      "122_result.json",
      "284_result.json",
      "198_result.json",
      "210_result.json",
      "260_result.json",
      "157_result.json",
      "109_result.json",
      "142_result.json",
      "132_result.json",
      "205_result.json",
      "288_result.json",
      "275_result.json",
      "100_result.json",
      "170_result.json",
      "103_result.json",
      "258_result.json",
      "206_result.json",
      "192_result.json",
      "234_result.json",
      "221_result.json",
      "282_result.json",
      "124_result.json",
      "138_result.json",
      "116_result.json",
      "182_result.json",
      "163_result.json",
      "113_result.json",
      "266_result.json",
      "158_result.json",
      "128_result.json",
      "203_result.json",
      "176_result.json",
      "106_result.json",
      "231_result.json",
      "197_result.json",
      "134_result.json",
      "144_result.json",
      "289_result.json",
      "171_result.json",
      "101_result.json",
      "190_result.json",
      "133_result.json",
      "218_result.json",
      "268_result.json",
      "280_result.json",
      "108_result.json",
      "253_result.json",
      "164_result.json",
      "114_result.json",
      "261_result.json",
      "211_result.json",
      "256_result.json",
      "180_result.json",
      "278_result.json",
      "208_result.json",
      "111_result.json",
      "104_result.json",
      "189_result.json",
      "201_result.json",
      "168_result.json",
      "243_result.json",
      "145_result.json",
      "135_result.json",
      "159_result.json",
      "107_result.json",
      "162_result.json",
      "286_result.json",
      "120_result.json",
      "225_result.json",
      "149_result.json",
      "262_result.json",
      "139_result.json",
      "212_result.json",
      "130_result.json",
      "235_result.json",
      "193_result.json",
      "172_result.json",
      "229_result.json"
    ],
    "Model": "Gemini 2.5 Pro",
    "Type": "Reasoning",
    "Source": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro",
    "Date": "2025-03-25",
    "Max Tokens": "30000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 51,
        "incorrect": 53,
        "accuracy": 49.03846153846153
      },
      "bound": {
        "correct": 30,
        "incorrect": 66,
        "accuracy": 31.25
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 32,
        "incorrect": 168,
        "accuracy": 16.0
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.230769230769234
      },
      "bound": {
        "correct": 12,
        "incorrect": 84,
        "accuracy": 12.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 20,
        "incorrect": 180,
        "accuracy": 10.0
      },
      "relation": {
        "correct": 12,
        "incorrect": 92,
        "accuracy": 11.538461538461538
      },
      "bound": {
        "correct": 8,
        "incorrect": 88,
        "accuracy": 8.333333333333332
      }
    },
    "approx_judge": {
      "total": {
        "correct": 119,
        "incorrect": 81,
        "accuracy": 59.5
      },
      "relation": {
        "correct": 55,
        "incorrect": 49,
        "accuracy": 52.88461538461539
      },
      "bound": {
        "correct": 64,
        "incorrect": 32,
        "accuracy": 66.66666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 5,
        "incorrect": 99,
        "accuracy": 4.807692307692308
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "all_true_indices": [
      "252_result.json",
      "289_result.json",
      "218_result.json",
      "261_result.json",
      "293_result.json"
    ],
    "Model": "GPT-4.1",
    "Type": "Chat",
    "Source": "https://platform.openai.com/docs/models/gpt-4.1",
    "Date": "2025-04-14",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 79,
        "incorrect": 121,
        "accuracy": 39.5
      },
      "relation": {
        "correct": 38,
        "incorrect": 66,
        "accuracy": 36.53846153846153
      },
      "bound": {
        "correct": 41,
        "incorrect": 55,
        "accuracy": 42.70833333333333
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 58,
        "incorrect": 142,
        "accuracy": 28.999999999999996
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.19230769230769
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.458333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "approx_judge": {
      "total": {
        "correct": 180,
        "incorrect": 20,
        "accuracy": 90.0
      },
      "relation": {
        "correct": 92,
        "incorrect": 12,
        "accuracy": 88.46153846153845
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.26923076923077
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 4,
        "incorrect": 196,
        "accuracy": 2.0
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0416666666666665
      }
    },
    "all_true_indices": [
      "210_result.json",
      "192_result.json",
      "253_result.json",
      "199_result.json"
    ],
    "Model": "GPT-4o mini",
    "Type": "Chat",
    "Source": "https://platform.openai.com/docs/models/gpt-4o-mini",
    "Date": "2024-07-18",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 75,
        "incorrect": 125,
        "accuracy": 37.5
      },
      "relation": {
        "correct": 36,
        "incorrect": 68,
        "accuracy": 34.61538461538461
      },
      "bound": {
        "correct": 39,
        "incorrect": 57,
        "accuracy": 40.625
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 64,
        "incorrect": 136,
        "accuracy": 32.0
      },
      "relation": {
        "correct": 43,
        "incorrect": 61,
        "accuracy": 41.34615384615385
      },
      "bound": {
        "correct": 21,
        "incorrect": 75,
        "accuracy": 21.875
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5000000000000004
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "approx_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "calc_judge": {
      "total": {
        "correct": 188,
        "incorrect": 12,
        "accuracy": 94.0
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "all_true": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "165_result.json",
      "210_result.json",
      "294_result.json",
      "192_result.json",
      "199_result.json",
      "261_result.json"
    ],
    "Model": "GPT-4o",
    "Type": "Chat",
    "Source": "https://platform.openai.com/docs/models/gpt-4o",
    "Date": "2024-08-06",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 109,
        "incorrect": 91,
        "accuracy": 54.50000000000001
      },
      "relation": {
        "correct": 45,
        "incorrect": 59,
        "accuracy": 43.269230769230774
      },
      "bound": {
        "correct": 64,
        "incorrect": 32,
        "accuracy": 66.66666666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 34,
        "incorrect": 166,
        "accuracy": 17.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.192307692307693
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.541666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 32,
        "incorrect": 168,
        "accuracy": 16.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.192307692307693
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.458333333333332
      }
    },
    "approx_judge": {
      "total": {
        "correct": 72,
        "incorrect": 128,
        "accuracy": 36.0
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.76923076923077
      },
      "bound": {
        "correct": 40,
        "incorrect": 56,
        "accuracy": 41.66666666666667
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 94,
        "incorrect": 10,
        "accuracy": 90.38461538461539
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5000000000000004
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "all_true_indices": [
      "291_result.json",
      "103_result.json",
      "258_result.json",
      "106_result.json",
      "253_result.json",
      "189_result.json",
      "149_result.json"
    ],
    "Model": "Grok 3",
    "Type": "Chat",
    "Source": "https://x.ai/news/grok-3",
    "Date": "2025-02-19",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 143,
        "incorrect": 57,
        "accuracy": 71.5
      },
      "relation": {
        "correct": 73,
        "incorrect": 31,
        "accuracy": 70.1923076923077
      },
      "bound": {
        "correct": 70,
        "incorrect": 26,
        "accuracy": 72.91666666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 48,
        "incorrect": 152,
        "accuracy": 24.0
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.76923076923077
      },
      "bound": {
        "correct": 16,
        "incorrect": 80,
        "accuracy": 16.666666666666664
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 39,
        "incorrect": 161,
        "accuracy": 19.5
      },
      "relation": {
        "correct": 28,
        "incorrect": 76,
        "accuracy": 26.923076923076923
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.458333333333332
      }
    },
    "approx_judge": {
      "total": {
        "correct": 107,
        "incorrect": 93,
        "accuracy": 53.5
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.230769230769226
      },
      "bound": {
        "correct": 61,
        "incorrect": 35,
        "accuracy": 63.541666666666664
      }
    },
    "calc_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 12,
        "incorrect": 188,
        "accuracy": 6.0
      },
      "relation": {
        "correct": 8,
        "incorrect": 96,
        "accuracy": 7.6923076923076925
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "all_true_indices": [
      "198_result.json",
      "191_result.json",
      "170_result.json",
      "103_result.json",
      "228_result.json",
      "248_result.json",
      "106_result.json",
      "289_result.json",
      "218_result.json",
      "253_result.json",
      "261_result.json",
      "293_result.json"
    ],
    "Model": "Grok 3 mini",
    "Type": "Reasoning",
    "Source": "https://x.ai/news/grok-3",
    "Date": "2025-02-19",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 125,
        "incorrect": 75,
        "accuracy": 62.5
      },
      "relation": {
        "correct": 65,
        "incorrect": 39,
        "accuracy": 62.5
      },
      "bound": {
        "correct": 60,
        "incorrect": 36,
        "accuracy": 62.5
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 69,
        "incorrect": 131,
        "accuracy": 34.5
      },
      "relation": {
        "correct": 33,
        "incorrect": 71,
        "accuracy": 31.73076923076923
      },
      "bound": {
        "correct": 36,
        "incorrect": 60,
        "accuracy": 37.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 35,
        "incorrect": 165,
        "accuracy": 17.5
      },
      "relation": {
        "correct": 23,
        "incorrect": 81,
        "accuracy": 22.115384615384613
      },
      "bound": {
        "correct": 12,
        "incorrect": 84,
        "accuracy": 12.5
      }
    },
    "approx_judge": {
      "total": {
        "correct": 173,
        "incorrect": 27,
        "accuracy": 86.5
      },
      "relation": {
        "correct": 78,
        "incorrect": 26,
        "accuracy": 75.0
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 199,
        "incorrect": 1,
        "accuracy": 99.5
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.03846153846155
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "all_true": {
      "total": {
        "correct": 16,
        "incorrect": 184,
        "accuracy": 8.0
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.653846153846153
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "all_true_indices": [
      "291_result.json",
      "281_result.json",
      "179_result.json",
      "228_result.json",
      "192_result.json",
      "266_result.json",
      "106_result.json",
      "289_result.json",
      "101_result.json",
      "253_result.json",
      "199_result.json",
      "293_result.json",
      "162_result.json",
      "120_result.json",
      "149_result.json",
      "235_result.json"
    ],
    "Model": "o1",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o1",
    "Date": "2024-12-17",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 136,
        "incorrect": 64,
        "accuracy": 68.0
      },
      "relation": {
        "correct": 69,
        "incorrect": 35,
        "accuracy": 66.34615384615384
      },
      "bound": {
        "correct": 67,
        "incorrect": 29,
        "accuracy": 69.79166666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 57,
        "incorrect": 143,
        "accuracy": 28.499999999999996
      },
      "relation": {
        "correct": 30,
        "incorrect": 74,
        "accuracy": 28.846153846153843
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.125
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 38,
        "incorrect": 162,
        "accuracy": 19.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.192307692307693
      },
      "bound": {
        "correct": 17,
        "incorrect": 79,
        "accuracy": 17.708333333333336
      }
    },
    "approx_judge": {
      "total": {
        "correct": 167,
        "incorrect": 33,
        "accuracy": 83.5
      },
      "relation": {
        "correct": 77,
        "incorrect": 27,
        "accuracy": 74.03846153846155
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "calc_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "all_true": {
      "total": {
        "correct": 15,
        "incorrect": 185,
        "accuracy": 7.5
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.653846153846153
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.25
      }
    },
    "all_true_indices": [
      "192_result.json",
      "221_result.json",
      "282_result.json",
      "266_result.json",
      "106_result.json",
      "289_result.json",
      "171_result.json",
      "253_result.json",
      "199_result.json",
      "114_result.json",
      "293_result.json",
      "162_result.json",
      "149_result.json",
      "235_result.json",
      "229_result.json"
    ],
    "Model": "o1",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o1",
    "Date": "2024-12-17",
    "Max Tokens": "40000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 125,
        "incorrect": 75,
        "accuracy": 62.5
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.769230769230774
      },
      "bound": {
        "correct": 67,
        "incorrect": 29,
        "accuracy": 69.79166666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 74,
        "incorrect": 126,
        "accuracy": 37.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.42307692307692
      },
      "bound": {
        "correct": 33,
        "incorrect": 63,
        "accuracy": 34.375
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 44,
        "incorrect": 156,
        "accuracy": 22.0
      },
      "relation": {
        "correct": 27,
        "incorrect": 77,
        "accuracy": 25.961538461538463
      },
      "bound": {
        "correct": 17,
        "incorrect": 79,
        "accuracy": 17.708333333333336
      }
    },
    "approx_judge": {
      "total": {
        "correct": 155,
        "incorrect": 45,
        "accuracy": 77.5
      },
      "relation": {
        "correct": 66,
        "incorrect": 38,
        "accuracy": 63.46153846153846
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.26923076923077
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "all_true": {
      "total": {
        "correct": 19,
        "incorrect": 181,
        "accuracy": 9.5
      },
      "relation": {
        "correct": 12,
        "incorrect": 92,
        "accuracy": 11.538461538461538
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "all_true_indices": [
      "291_result.json",
      "284_result.json",
      "198_result.json",
      "252_result.json",
      "228_result.json",
      "192_result.json",
      "234_result.json",
      "221_result.json",
      "166_result.json",
      "106_result.json",
      "289_result.json",
      "190_result.json",
      "253_result.json",
      "243_result.json",
      "135_result.json",
      "162_result.json",
      "149_result.json",
      "296_result.json",
      "229_result.json"
    ],
    "Model": "o3-mini",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o3-mini",
    "Date": "2025-01-31",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 136,
        "incorrect": 64,
        "accuracy": 68.0
      },
      "relation": {
        "correct": 74,
        "incorrect": 30,
        "accuracy": 71.15384615384616
      },
      "bound": {
        "correct": 62,
        "incorrect": 34,
        "accuracy": 64.58333333333334
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 197,
        "incorrect": 3,
        "accuracy": 98.5
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.03846153846155
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 134,
        "incorrect": 66,
        "accuracy": 67.0
      },
      "relation": {
        "correct": 76,
        "incorrect": 28,
        "accuracy": 73.07692307692307
      },
      "bound": {
        "correct": 58,
        "incorrect": 38,
        "accuracy": 60.416666666666664
      }
    },
    "approx_judge": {
      "total": {
        "correct": 174,
        "incorrect": 26,
        "accuracy": 87.0
      },
      "relation": {
        "correct": 81,
        "incorrect": 23,
        "accuracy": 77.88461538461539
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "calc_judge": {
      "total": {
        "correct": 195,
        "incorrect": 5,
        "accuracy": 97.5
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.07692307692307
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "all_true": {
      "total": {
        "correct": 91,
        "incorrect": 109,
        "accuracy": 45.5
      },
      "relation": {
        "correct": 48,
        "incorrect": 56,
        "accuracy": 46.15384615384615
      },
      "bound": {
        "correct": 43,
        "incorrect": 53,
        "accuracy": 44.79166666666667
      }
    },
    "all_true_indices": [
      "100_result.json",
      "101_result.json",
      "103_result.json",
      "105_result.json",
      "106_result.json",
      "107_result.json",
      "108_result.json",
      "113_result.json",
      "114_result.json",
      "116_result.json",
      "119_result.json",
      "120_result.json",
      "121_result.json",
      "122_result.json",
      "130_result.json",
      "132_result.json",
      "133_result.json",
      "134_result.json",
      "135_result.json",
      "137_result.json",
      "139_result.json",
      "142_result.json",
      "145_result.json",
      "147_result.json",
      "149_result.json",
      "152_result.json",
      "155_result.json",
      "156_result.json",
      "157_result.json",
      "158_result.json",
      "159_result.json",
      "165_result.json",
      "168_result.json",
      "171_result.json",
      "173_result.json",
      "175_result.json",
      "176_result.json",
      "180_result.json",
      "182_result.json",
      "190_result.json",
      "192_result.json",
      "193_result.json",
      "194_result.json",
      "197_result.json",
      "198_result.json",
      "199_result.json",
      "200_result.json",
      "202_result.json",
      "205_result.json",
      "207_result.json",
      "209_result.json",
      "210_result.json",
      "218_result.json",
      "219_result.json",
      "220_result.json",
      "221_result.json",
      "229_result.json",
      "232_result.json",
      "233_result.json",
      "234_result.json",
      "237_result.json",
      "243_result.json",
      "249_result.json",
      "252_result.json",
      "253_result.json",
      "256_result.json",
      "257_result.json",
      "258_result.json",
      "260_result.json",
      "261_result.json",
      "262_result.json",
      "264_result.json",
      "266_result.json",
      "268_result.json",
      "269_result.json",
      "275_result.json",
      "276_result.json",
      "278_result.json",
      "280_result.json",
      "282_result.json",
      "284_result.json",
      "286_result.json",
      "288_result.json",
      "289_result.json",
      "290_result.json",
      "291_result.json",
      "293_result.json",
      "294_result.json",
      "297_result.json",
      "298_result.json",
      "203_result.json"
    ],
    "Model": "o3-pro",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o3-pro",
    "Date": "2025-06-10",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 137,
        "incorrect": 62,
        "accuracy": 68.5
      },
      "relation": {
        "correct": 74,
        "incorrect": 30,
        "accuracy": 71.15384615384616
      },
      "bound": {
        "correct": 63,
        "incorrect": 32,
        "accuracy": 66.3157894736842
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 190,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 91,
        "incorrect": 4,
        "accuracy": 95.78947368421052
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 147,
        "incorrect": 52,
        "accuracy": 73.5
      },
      "relation": {
        "correct": 82,
        "incorrect": 22,
        "accuracy": 78.84615384615384
      },
      "bound": {
        "correct": 65,
        "incorrect": 30,
        "accuracy": 68.42105263157895
      }
    },
    "approx_judge": {
      "total": {
        "correct": 172,
        "incorrect": 27,
        "accuracy": 86.0
      },
      "relation": {
        "correct": 82,
        "incorrect": 22,
        "accuracy": 78.84615384615384
      },
      "bound": {
        "correct": 90,
        "incorrect": 5,
        "accuracy": 94.73684210526315
      }
    },
    "calc_judge": {
      "total": {
        "correct": 189,
        "incorrect": 10,
        "accuracy": 94.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 91,
        "incorrect": 4,
        "accuracy": 95.78947368421052
      }
    },
    "all_true": {
      "total": {
        "correct": 92,
        "incorrect": 107,
        "accuracy": 46.0
      },
      "relation": {
        "correct": 45,
        "incorrect": 59,
        "accuracy": 43.269230769230774
      },
      "bound": {
        "correct": 47,
        "incorrect": 48,
        "accuracy": 49.473684210526315
      }
    },
    "all_true_indices": [
      "106_result.json",
      "119_result.json",
      "101_result.json",
      "116_result.json",
      "103_result.json",
      "113_result.json",
      "105_result.json",
      "108_result.json",
      "114_result.json",
      "107_result.json",
      "100_result.json",
      "126_result.json",
      "121_result.json",
      "120_result.json",
      "130_result.json",
      "132_result.json",
      "135_result.json",
      "144_result.json",
      "149_result.json",
      "159_result.json",
      "142_result.json",
      "155_result.json",
      "158_result.json",
      "156_result.json",
      "157_result.json",
      "168_result.json",
      "170_result.json",
      "164_result.json",
      "173_result.json",
      "171_result.json",
      "162_result.json",
      "176_result.json",
      "160_result.json",
      "179_result.json",
      "166_result.json",
      "180_result.json",
      "181_result.json",
      "175_result.json",
      "190_result.json",
      "186_result.json",
      "183_result.json",
      "188_result.json",
      "192_result.json",
      "189_result.json",
      "191_result.json",
      "197_result.json",
      "178_result.json",
      "199_result.json",
      "198_result.json",
      "194_result.json",
      "207_result.json",
      "204_result.json",
      "200_result.json",
      "211_result.json",
      "212_result.json",
      "210_result.json",
      "219_result.json",
      "201_result.json",
      "220_result.json",
      "221_result.json",
      "229_result.json",
      "234_result.json",
      "225_result.json",
      "232_result.json",
      "237_result.json",
      "240_result.json",
      "257_result.json",
      "256_result.json",
      "252_result.json",
      "253_result.json",
      "261_result.json",
      "260_result.json",
      "258_result.json",
      "264_result.json",
      "265_result.json",
      "268_result.json",
      "262_result.json",
      "275_result.json",
      "276_result.json",
      "278_result.json",
      "266_result.json",
      "280_result.json",
      "282_result.json",
      "289_result.json",
      "290_result.json",
      "284_result.json",
      "293_result.json",
      "286_result.json",
      "291_result.json",
      "285_result.json",
      "298_result.json",
      "299_result.json"
    ],
    "Model": "o3-pro",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o3-pro",
    "Date": "2025-06-10",
    "Max Tokens": "40000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 74,
        "incorrect": 126,
        "accuracy": 37.0
      },
      "relation": {
        "correct": 45,
        "incorrect": 59,
        "accuracy": 43.269230769230774
      },
      "bound": {
        "correct": 29,
        "incorrect": 67,
        "accuracy": 30.208333333333332
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 79,
        "incorrect": 121,
        "accuracy": 39.5
      },
      "relation": {
        "correct": 52,
        "incorrect": 52,
        "accuracy": 50.0
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.125
      }
    },
    "approx_judge": {
      "total": {
        "correct": 183,
        "incorrect": 17,
        "accuracy": 91.5
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.61538461538461
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 194,
        "incorrect": 6,
        "accuracy": 97.0
      },
      "relation": {
        "correct": 101,
        "incorrect": 3,
        "accuracy": 97.11538461538461
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "all_true": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 24,
        "incorrect": 80,
        "accuracy": 23.076923076923077
      },
      "bound": {
        "correct": 18,
        "incorrect": 78,
        "accuracy": 18.75
      }
    },
    "all_true_indices": [
      "291_result.json",
      "232_result.json",
      "194_result.json",
      "298_result.json",
      "152_result.json",
      "198_result.json",
      "210_result.json",
      "260_result.json",
      "184_result.json",
      "237_result.json",
      "142_result.json",
      "132_result.json",
      "275_result.json",
      "103_result.json",
      "258_result.json",
      "192_result.json",
      "234_result.json",
      "221_result.json",
      "282_result.json",
      "166_result.json",
      "176_result.json",
      "106_result.json",
      "197_result.json",
      "289_result.json",
      "101_result.json",
      "190_result.json",
      "218_result.json",
      "108_result.json",
      "253_result.json",
      "199_result.json",
      "114_result.json",
      "261_result.json",
      "243_result.json",
      "196_result.json",
      "293_result.json",
      "135_result.json",
      "107_result.json",
      "162_result.json",
      "149_result.json",
      "262_result.json",
      "212_result.json",
      "229_result.json"
    ],
    "Model": "o3",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o3",
    "Date": "2025-04-16",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 144,
        "incorrect": 56,
        "accuracy": 72.0
      },
      "relation": {
        "correct": 76,
        "incorrect": 28,
        "accuracy": 73.07692307692307
      },
      "bound": {
        "correct": 68,
        "incorrect": 28,
        "accuracy": 70.83333333333334
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 193,
        "incorrect": 7,
        "accuracy": 96.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 112,
        "incorrect": 88,
        "accuracy": 56.00000000000001
      },
      "relation": {
        "correct": 60,
        "incorrect": 44,
        "accuracy": 57.692307692307686
      },
      "bound": {
        "correct": 52,
        "incorrect": 44,
        "accuracy": 54.166666666666664
      }
    },
    "approx_judge": {
      "total": {
        "correct": 173,
        "incorrect": 27,
        "accuracy": 86.5
      },
      "relation": {
        "correct": 81,
        "incorrect": 23,
        "accuracy": 77.88461538461539
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 188,
        "incorrect": 12,
        "accuracy": 94.0
      },
      "relation": {
        "correct": 96,
        "incorrect": 8,
        "accuracy": 92.3076923076923
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 74,
        "incorrect": 126,
        "accuracy": 37.0
      },
      "relation": {
        "correct": 35,
        "incorrect": 69,
        "accuracy": 33.65384615384615
      },
      "bound": {
        "correct": 39,
        "incorrect": 57,
        "accuracy": 40.625
      }
    },
    "all_true_indices": [
      "147_result.json",
      "291_result.json",
      "232_result.json",
      "119_result.json",
      "175_result.json",
      "188_result.json",
      "298_result.json",
      "215_result.json",
      "181_result.json",
      "257_result.json",
      "284_result.json",
      "198_result.json",
      "210_result.json",
      "260_result.json",
      "127_result.json",
      "179_result.json",
      "184_result.json",
      "252_result.json",
      "109_result.json",
      "142_result.json",
      "132_result.json",
      "205_result.json",
      "100_result.json",
      "103_result.json",
      "173_result.json",
      "258_result.json",
      "206_result.json",
      "297_result.json",
      "192_result.json",
      "234_result.json",
      "221_result.json",
      "282_result.json",
      "166_result.json",
      "248_result.json",
      "266_result.json",
      "176_result.json",
      "106_result.json",
      "197_result.json",
      "134_result.json",
      "289_result.json",
      "171_result.json",
      "101_result.json",
      "133_result.json",
      "218_result.json",
      "268_result.json",
      "280_result.json",
      "156_result.json",
      "108_result.json",
      "253_result.json",
      "199_result.json",
      "114_result.json",
      "261_result.json",
      "256_result.json",
      "180_result.json",
      "278_result.json",
      "111_result.json",
      "104_result.json",
      "189_result.json",
      "290_result.json",
      "168_result.json",
      "243_result.json",
      "293_result.json",
      "135_result.json",
      "159_result.json",
      "107_result.json",
      "162_result.json",
      "120_result.json",
      "225_result.json",
      "149_result.json",
      "212_result.json",
      "186_result.json",
      "130_result.json",
      "193_result.json",
      "229_result.json"
    ],
    "Model": "o3",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o3",
    "Date": "2025-04-16",
    "Max Tokens": "40000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 130,
        "incorrect": 70,
        "accuracy": 65.0
      },
      "relation": {
        "correct": 69,
        "incorrect": 35,
        "accuracy": 66.34615384615384
      },
      "bound": {
        "correct": 61,
        "incorrect": 35,
        "accuracy": 63.541666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 124,
        "incorrect": 76,
        "accuracy": 62.0
      },
      "relation": {
        "correct": 68,
        "incorrect": 36,
        "accuracy": 65.38461538461539
      },
      "bound": {
        "correct": 56,
        "incorrect": 40,
        "accuracy": 58.333333333333336
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 52,
        "incorrect": 148,
        "accuracy": 26.0
      },
      "relation": {
        "correct": 28,
        "incorrect": 76,
        "accuracy": 26.923076923076923
      },
      "bound": {
        "correct": 24,
        "incorrect": 72,
        "accuracy": 25.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 173,
        "incorrect": 27,
        "accuracy": 86.5
      },
      "relation": {
        "correct": 86,
        "incorrect": 18,
        "accuracy": 82.6923076923077
      },
      "bound": {
        "correct": 87,
        "incorrect": 9,
        "accuracy": 90.625
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.26923076923077
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 31,
        "incorrect": 169,
        "accuracy": 15.5
      },
      "relation": {
        "correct": 17,
        "incorrect": 87,
        "accuracy": 16.346153846153847
      },
      "bound": {
        "correct": 14,
        "incorrect": 82,
        "accuracy": 14.583333333333334
      }
    },
    "all_true_indices": [
      "291_result.json",
      "181_result.json",
      "198_result.json",
      "210_result.json",
      "281_result.json",
      "179_result.json",
      "184_result.json",
      "252_result.json",
      "275_result.json",
      "192_result.json",
      "282_result.json",
      "166_result.json",
      "151_result.json",
      "224_result.json",
      "176_result.json",
      "106_result.json",
      "289_result.json",
      "101_result.json",
      "190_result.json",
      "280_result.json",
      "253_result.json",
      "199_result.json",
      "261_result.json",
      "123_result.json",
      "189_result.json",
      "135_result.json",
      "239_result.json",
      "162_result.json",
      "296_result.json",
      "235_result.json",
      "229_result.json"
    ],
    "Model": "o4-mini",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o4-mini",
    "Date": "2025-04-16",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 107,
        "incorrect": 93,
        "accuracy": 53.5
      },
      "relation": {
        "correct": 51,
        "incorrect": 53,
        "accuracy": 49.03846153846153
      },
      "bound": {
        "correct": 56,
        "incorrect": 40,
        "accuracy": 58.333333333333336
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 46,
        "incorrect": 154,
        "accuracy": 23.0
      },
      "relation": {
        "correct": 23,
        "incorrect": 81,
        "accuracy": 22.115384615384613
      },
      "bound": {
        "correct": 23,
        "incorrect": 73,
        "accuracy": 23.958333333333336
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 52,
        "incorrect": 148,
        "accuracy": 26.0
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.76923076923077
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.833333333333336
      }
    },
    "approx_judge": {
      "total": {
        "correct": 71,
        "incorrect": 129,
        "accuracy": 35.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.69230769230769
      },
      "bound": {
        "correct": 37,
        "incorrect": 59,
        "accuracy": 38.54166666666667
      }
    },
    "calc_judge": {
      "total": {
        "correct": 174,
        "incorrect": 26,
        "accuracy": 87.0
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.61538461538461
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.58333333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5000000000000004
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 5,
        "incorrect": 91,
        "accuracy": 5.208333333333334
      }
    },
    "all_true_indices": [
      "184_result.json",
      "221_result.json",
      "124_result.json",
      "101_result.json",
      "261_result.json",
      "162_result.json",
      "149_result.json"
    ],
    "Model": "DeepSeek-R1 (Llama-70B)",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
    "Date": "2025-01-20",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "70B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 29,
        "incorrect": 171,
        "accuracy": 14.499999999999998
      },
      "relation": {
        "correct": 13,
        "incorrect": 91,
        "accuracy": 12.5
      },
      "bound": {
        "correct": 16,
        "incorrect": 80,
        "accuracy": 16.666666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 40,
        "incorrect": 160,
        "accuracy": 20.0
      },
      "relation": {
        "correct": 29,
        "incorrect": 75,
        "accuracy": 27.884615384615387
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.458333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 12,
        "incorrect": 188,
        "accuracy": 6.0
      },
      "relation": {
        "correct": 7,
        "incorrect": 97,
        "accuracy": 6.730769230769231
      },
      "bound": {
        "correct": 5,
        "incorrect": 91,
        "accuracy": 5.208333333333334
      }
    },
    "approx_judge": {
      "total": {
        "correct": 96,
        "incorrect": 104,
        "accuracy": 48.0
      },
      "relation": {
        "correct": 49,
        "incorrect": 55,
        "accuracy": 47.11538461538461
      },
      "bound": {
        "correct": 47,
        "incorrect": 49,
        "accuracy": 48.95833333333333
      }
    },
    "calc_judge": {
      "total": {
        "correct": 167,
        "incorrect": 33,
        "accuracy": 83.5
      },
      "relation": {
        "correct": 83,
        "incorrect": 21,
        "accuracy": 79.8076923076923
      },
      "bound": {
        "correct": 84,
        "incorrect": 12,
        "accuracy": 87.5
      }
    },
    "all_true": {
      "total": {
        "correct": 1,
        "incorrect": 199,
        "accuracy": 0.5
      },
      "relation": {
        "correct": 1,
        "incorrect": 103,
        "accuracy": 0.9615384615384616
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "all_true_indices": [
      "282_result.json"
    ],
    "Model": "DeepSeek-R1 (Qwen-1.5B)",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "Date": "2025-01-20",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "1.5B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 40,
        "incorrect": 64,
        "accuracy": 38.46153846153847
      },
      "bound": {
        "correct": 41,
        "incorrect": 55,
        "accuracy": 42.70833333333333
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 24,
        "incorrect": 80,
        "accuracy": 23.076923076923077
      },
      "bound": {
        "correct": 18,
        "incorrect": 78,
        "accuracy": 18.75
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 23,
        "incorrect": 81,
        "accuracy": 22.115384615384613
      },
      "bound": {
        "correct": 19,
        "incorrect": 77,
        "accuracy": 19.791666666666664
      }
    },
    "approx_judge": {
      "total": {
        "correct": 71,
        "incorrect": 129,
        "accuracy": 35.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.69230769230769
      },
      "bound": {
        "correct": 37,
        "incorrect": 59,
        "accuracy": 38.54166666666667
      }
    },
    "calc_judge": {
      "total": {
        "correct": 170,
        "incorrect": 30,
        "accuracy": 85.0
      },
      "relation": {
        "correct": 82,
        "incorrect": 22,
        "accuracy": 78.84615384615384
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 10,
        "incorrect": 190,
        "accuracy": 5.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.25
      }
    },
    "all_true_indices": [
      "221_result.json",
      "166_result.json",
      "106_result.json",
      "253_result.json",
      "261_result.json",
      "285_result.json",
      "123_result.json",
      "189_result.json",
      "162_result.json",
      "149_result.json"
    ],
    "Model": "DeepSeek-R1 (Qwen-14B)",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "Date": "2025-01-20",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "14B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 22,
        "incorrect": 178,
        "accuracy": 11.0
      },
      "relation": {
        "correct": 15,
        "incorrect": 89,
        "accuracy": 14.423076923076922
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 164,
        "incorrect": 36,
        "accuracy": 82.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 69,
        "incorrect": 27,
        "accuracy": 71.875
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 197,
        "incorrect": 3,
        "accuracy": 98.5
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.07692307692307
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 177,
        "incorrect": 23,
        "accuracy": 88.5
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.61538461538461
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "all_true_indices": [],
    "Model": "Llama-3.2-3B",
    "Type": "Chat",
    "Source": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",
    "Date": "2024-09-25",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "3B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 37,
        "incorrect": 67,
        "accuracy": 35.57692307692308
      },
      "bound": {
        "correct": 44,
        "incorrect": 52,
        "accuracy": 45.83333333333333
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 85,
        "incorrect": 115,
        "accuracy": 42.5
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.769230769230774
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.125
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 8,
        "incorrect": 192,
        "accuracy": 4.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "approx_judge": {
      "total": {
        "correct": 178,
        "incorrect": 22,
        "accuracy": 89.0
      },
      "relation": {
        "correct": 90,
        "incorrect": 14,
        "accuracy": 86.53846153846155
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 101,
        "incorrect": 3,
        "accuracy": 97.11538461538461
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "198_result.json",
      "103_result.json",
      "258_result.json",
      "106_result.json",
      "274_result.json"
    ],
    "Model": "Llama-4-Maverick",
    "Type": "Chat",
    "Source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
    "Date": "2025-04-05",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "128 x 17B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 67,
        "incorrect": 133,
        "accuracy": 33.5
      },
      "relation": {
        "correct": 22,
        "incorrect": 82,
        "accuracy": 21.153846153846153
      },
      "bound": {
        "correct": 45,
        "incorrect": 51,
        "accuracy": 46.875
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 61,
        "incorrect": 139,
        "accuracy": 30.5
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.230769230769226
      },
      "bound": {
        "correct": 15,
        "incorrect": 81,
        "accuracy": 15.625
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5000000000000004
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "approx_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 96,
        "incorrect": 8,
        "accuracy": 92.3076923076923
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 3,
        "incorrect": 197,
        "accuracy": 1.5
      },
      "relation": {
        "correct": 1,
        "incorrect": 103,
        "accuracy": 0.9615384615384616
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "228_result.json",
      "192_result.json",
      "106_result.json"
    ],
    "Model": "Llama-4-Scout",
    "Type": "Chat",
    "Source": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
    "Date": "2025-04-05",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "16 x 17B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 29,
        "incorrect": 171,
        "accuracy": 14.499999999999998
      },
      "relation": {
        "correct": 23,
        "incorrect": 81,
        "accuracy": 22.115384615384613
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.25
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.15384615384616
      },
      "bound": {
        "correct": 81,
        "incorrect": 15,
        "accuracy": 84.375
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 198,
        "incorrect": 2,
        "accuracy": 99.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 184,
        "incorrect": 16,
        "accuracy": 92.0
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 85,
        "incorrect": 11,
        "accuracy": 88.54166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "all_true_indices": [],
    "Model": "Llama-3.1-8B",
    "Type": "Chat",
    "Source": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
    "Date": "2024-07-18",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "8B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 87,
        "incorrect": 113,
        "accuracy": 43.5
      },
      "relation": {
        "correct": 42,
        "incorrect": 62,
        "accuracy": 40.38461538461539
      },
      "bound": {
        "correct": 45,
        "incorrect": 51,
        "accuracy": 46.875
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 56,
        "incorrect": 144,
        "accuracy": 28.000000000000004
      },
      "relation": {
        "correct": 31,
        "incorrect": 73,
        "accuracy": 29.807692307692307
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.041666666666668
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 60,
        "incorrect": 140,
        "accuracy": 30.0
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.69230769230769
      },
      "bound": {
        "correct": 26,
        "incorrect": 70,
        "accuracy": 27.083333333333332
      }
    },
    "approx_judge": {
      "total": {
        "correct": 45,
        "incorrect": 155,
        "accuracy": 22.5
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.230769230769234
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.041666666666668
      }
    },
    "calc_judge": {
      "total": {
        "correct": 175,
        "incorrect": 25,
        "accuracy": 87.5
      },
      "relation": {
        "correct": 94,
        "incorrect": 10,
        "accuracy": 90.38461538461539
      },
      "bound": {
        "correct": 81,
        "incorrect": 15,
        "accuracy": 84.375
      }
    },
    "all_true": {
      "total": {
        "correct": 4,
        "incorrect": 196,
        "accuracy": 2.0
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "284_result.json",
      "252_result.json",
      "106_result.json",
      "149_result.json"
    ],
    "Model": "QwQ-32B-preview",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/Qwen/QwQ-32B-Preview",
    "Date": "2024-11-27",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "32B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 99,
        "incorrect": 101,
        "accuracy": 49.5
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.19230769230769
      },
      "bound": {
        "correct": 52,
        "incorrect": 44,
        "accuracy": 54.166666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 52,
        "incorrect": 148,
        "accuracy": 26.0
      },
      "relation": {
        "correct": 28,
        "incorrect": 76,
        "accuracy": 26.923076923076923
      },
      "bound": {
        "correct": 24,
        "incorrect": 72,
        "accuracy": 25.0
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 59,
        "incorrect": 141,
        "accuracy": 29.5
      },
      "relation": {
        "correct": 39,
        "incorrect": 65,
        "accuracy": 37.5
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.833333333333336
      }
    },
    "approx_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 22,
        "incorrect": 82,
        "accuracy": 21.153846153846153
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.833333333333336
      }
    },
    "calc_judge": {
      "total": {
        "correct": 174,
        "incorrect": 26,
        "accuracy": 87.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 79,
        "incorrect": 17,
        "accuracy": 82.29166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 4,
        "incorrect": 196,
        "accuracy": 2.0
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "275_result.json",
      "182_result.json",
      "261_result.json",
      "149_result.json"
    ],
    "Model": "QwQ-32B",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/Qwen/QwQ-32B",
    "Date": "2025-03-05",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "32B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 84,
        "incorrect": 116,
        "accuracy": 42.0
      },
      "relation": {
        "correct": 35,
        "incorrect": 69,
        "accuracy": 33.65384615384615
      },
      "bound": {
        "correct": 49,
        "incorrect": 47,
        "accuracy": 51.041666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 109,
        "incorrect": 91,
        "accuracy": 54.50000000000001
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.769230769230774
      },
      "bound": {
        "correct": 51,
        "incorrect": 45,
        "accuracy": 53.125
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 10,
        "incorrect": 190,
        "accuracy": 5.0
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "approx_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "all_true_indices": [
      "103_result.json",
      "154_result.json",
      "163_result.json",
      "253_result.json",
      "293_result.json"
    ],
    "Model": "Qwen2.5-72B",
    "Type": "Chat",
    "Source": "https://huggingface.co/Qwen/Qwen2.5-72B-Instruct",
    "Date": "2024-09-16",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "72B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 70,
        "incorrect": 130,
        "accuracy": 35.0
      },
      "relation": {
        "correct": 31,
        "incorrect": 73,
        "accuracy": 29.807692307692307
      },
      "bound": {
        "correct": 39,
        "incorrect": 57,
        "accuracy": 40.625
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 89,
        "incorrect": 111,
        "accuracy": 44.5
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.769230769230774
      },
      "bound": {
        "correct": 31,
        "incorrect": 65,
        "accuracy": 32.29166666666667
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 9,
        "incorrect": 191,
        "accuracy": 4.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "approx_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 92,
        "incorrect": 12,
        "accuracy": 88.46153846153845
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.26923076923077
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "192_result.json",
      "203_result.json",
      "106_result.json",
      "253_result.json",
      "261_result.json",
      "229_result.json"
    ],
    "Model": "Qwen2.5-7B",
    "Type": "Chat",
    "Source": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
    "Date": "2024-09-16",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "7B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.76923076923077
      },
      "bound": {
        "correct": 49,
        "incorrect": 47,
        "accuracy": 51.041666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 72,
        "incorrect": 128,
        "accuracy": 36.0
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.230769230769226
      },
      "bound": {
        "correct": 26,
        "incorrect": 70,
        "accuracy": 27.083333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "approx_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.61538461538461
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "calc_judge": {
      "total": {
        "correct": 177,
        "incorrect": 23,
        "accuracy": 88.5
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.58333333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 3,
        "incorrect": 197,
        "accuracy": 1.5
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0416666666666665
      }
    },
    "all_true_indices": [
      "210_result.json",
      "192_result.json",
      "253_result.json"
    ],
    "Model": "Qwen2.5-Coder-32B",
    "Type": "Chat",
    "Source": "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct",
    "Date": "2024-11-10",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "32B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 82,
        "incorrect": 118,
        "accuracy": 41.0
      },
      "relation": {
        "correct": 48,
        "incorrect": 56,
        "accuracy": 46.15384615384615
      },
      "bound": {
        "correct": 34,
        "incorrect": 62,
        "accuracy": 35.41666666666667
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 70,
        "incorrect": 130,
        "accuracy": 35.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.42307692307692
      },
      "bound": {
        "correct": 29,
        "incorrect": 67,
        "accuracy": 30.208333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 72,
        "incorrect": 128,
        "accuracy": 36.0
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.19230769230769
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.041666666666668
      }
    },
    "approx_judge": {
      "total": {
        "correct": 62,
        "incorrect": 138,
        "accuracy": 31.0
      },
      "relation": {
        "correct": 35,
        "incorrect": 69,
        "accuracy": 33.65384615384615
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.125
      }
    },
    "calc_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "all_true": {
      "total": {
        "correct": 12,
        "incorrect": 188,
        "accuracy": 6.0
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.653846153846153
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "all_true_indices": [
      "291_result.json",
      "284_result.json",
      "275_result.json",
      "253_result.json",
      "261_result.json",
      "123_result.json",
      "189_result.json",
      "243_result.json",
      "293_result.json",
      "149_result.json",
      "262_result.json",
      "229_result.json"
    ],
    "Model": "Qwen3-235B-A22B",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/Qwen/Qwen3-235B-A22B-FP8",
    "Date": "2025-04-28",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "235B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 31,
        "incorrect": 169,
        "accuracy": 15.5
      },
      "relation": {
        "correct": 25,
        "incorrect": 79,
        "accuracy": 24.03846153846154
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.25
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 167,
        "incorrect": 33,
        "accuracy": 83.5
      },
      "relation": {
        "correct": 96,
        "incorrect": 8,
        "accuracy": 92.3076923076923
      },
      "bound": {
        "correct": 71,
        "incorrect": 25,
        "accuracy": 73.95833333333334
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 1,
        "incorrect": 199,
        "accuracy": 0.5
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0416666666666665
      }
    },
    "approx_judge": {
      "total": {
        "correct": 200,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 198,
        "incorrect": 2,
        "accuracy": 99.0
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.03846153846155
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "all_true_indices": [],
    "Model": "Gemma-2-9B",
    "Type": "Chat",
    "Source": "https://huggingface.co/google/gemma-2-9b-it",
    "Date": "2024-06-25",
    "Max Tokens": "6000",
    "Model Source": "Open-source",
    "Size": "9B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 15,
        "incorrect": 185,
        "accuracy": 7.5
      },
      "relation": {
        "correct": 11,
        "incorrect": 93,
        "accuracy": 10.576923076923077
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 147,
        "incorrect": 53,
        "accuracy": 73.5
      },
      "relation": {
        "correct": 83,
        "incorrect": 21,
        "accuracy": 79.8076923076923
      },
      "bound": {
        "correct": 64,
        "incorrect": 32,
        "accuracy": 66.66666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 198,
        "incorrect": 2,
        "accuracy": 99.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.03846153846155
      },
      "bound": {
        "correct": 87,
        "incorrect": 9,
        "accuracy": 90.625
      }
    },
    "all_true": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "all_true_indices": [],
    "Model": "Gemma-2B",
    "Type": "Chat",
    "Source": "https://huggingface.co/google/gemma-2b-it",
    "Date": "2024-02-21",
    "Max Tokens": "6000",
    "Model Source": "Open-source",
    "Size": "2B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 146,
        "incorrect": 54,
        "accuracy": 73.0
      },
      "relation": {
        "correct": 73,
        "incorrect": 31,
        "accuracy": 70.1923076923077
      },
      "bound": {
        "correct": 73,
        "incorrect": 23,
        "accuracy": 76.04166666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 104,
        "incorrect": 96,
        "accuracy": 52.0
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.769230769230774
      },
      "bound": {
        "correct": 46,
        "incorrect": 50,
        "accuracy": 47.91666666666667
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 62,
        "incorrect": 138,
        "accuracy": 31.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.42307692307692
      },
      "bound": {
        "correct": 21,
        "incorrect": 75,
        "accuracy": 21.875
      }
    },
    "approx_judge": {
      "total": {
        "correct": 129,
        "incorrect": 71,
        "accuracy": 64.5
      },
      "relation": {
        "correct": 60,
        "incorrect": 44,
        "accuracy": 57.692307692307686
      },
      "bound": {
        "correct": 69,
        "incorrect": 27,
        "accuracy": 71.875
      }
    },
    "calc_judge": {
      "total": {
        "correct": 189,
        "incorrect": 11,
        "accuracy": 94.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 31,
        "incorrect": 169,
        "accuracy": 15.5
      },
      "relation": {
        "correct": 18,
        "incorrect": 86,
        "accuracy": 17.307692307692307
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.541666666666666
      }
    },
    "all_true_indices": [
      "6",
      "7",
      "0",
      "14",
      "19",
      "28",
      "31",
      "35",
      "49",
      "66",
      "76",
      "87",
      "92",
      "99",
      "98",
      "121",
      "120",
      "129",
      "134",
      "143",
      "153",
      "157",
      "161",
      "158",
      "160",
      "175",
      "185",
      "188",
      "186",
      "193",
      "198"
    ],
    "Model": "DeepSeek-V3.1 (Thinking Mode)",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "671B",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
    "Reasoning effort": "not applicable",
    "Date": "2025-08-21",
    "Max Tokens": 30000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-08-24 02:44:05.611544"
  },
  {
    "is_correct": {
      "total": {
        "correct": 65,
        "incorrect": 135,
        "accuracy": 32.5
      },
      "relation": {
        "correct": 49,
        "incorrect": 55,
        "accuracy": 47.11538461538461
      },
      "bound": {
        "correct": 16,
        "incorrect": 80,
        "accuracy": 16.666666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 153,
        "incorrect": 47,
        "accuracy": 76.5
      },
      "relation": {
        "correct": 74,
        "incorrect": 30,
        "accuracy": 71.15384615384616
      },
      "bound": {
        "correct": 79,
        "incorrect": 17,
        "accuracy": 82.29166666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 54,
        "incorrect": 146,
        "accuracy": 27.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.42307692307692
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.541666666666666
      }
    },
    "approx_judge": {
      "total": {
        "correct": 176,
        "incorrect": 24,
        "accuracy": 88.0
      },
      "relation": {
        "correct": 81,
        "incorrect": 23,
        "accuracy": 77.88461538461539
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 195,
        "incorrect": 5,
        "accuracy": 97.5
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.15384615384616
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 24,
        "incorrect": 176,
        "accuracy": 12.0
      },
      "relation": {
        "correct": 15,
        "incorrect": 89,
        "accuracy": 14.423076923076922
      },
      "bound": {
        "correct": 9,
        "incorrect": 87,
        "accuracy": 9.375
      }
    },
    "all_true_indices": [
      "3",
      "6",
      "14",
      "23",
      "28",
      "49",
      "66",
      "76",
      "92",
      "99",
      "98",
      "105",
      "121",
      "129",
      "134",
      "153",
      "160",
      "161",
      "166",
      "175",
      "193",
      "189",
      "188",
      "198"
    ],
    "Model": "DeepSeek-V3.1 (Thinking Mode)",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "671B",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
    "Reasoning effort": "not applicable",
    "Date": "2025-08-21",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "leaderboard": true,
    "timestamp": "2025-08-24 03:14:42.936489"
  }
]